# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18uHtL4s5MZ90jaBkP2E8qMwN_nkvw7_X
"""

pip install pandas numpy matplotlib seaborn scikit-learn xgboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("pastel")

# --- 1. Load Data ---
try:
    df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')
except FileNotFoundError:
    print("Error: file not found.")
    exit()

print("--- Initial Data Head ---")
print(df.head())
print("\n--- Data Info ---")
df.info()

# --- 2. Data Cleaning and Preprocessing ---

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df = df.drop('customerID', axis=1)
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

df['SeniorCitizen'] = df['SeniorCitizen'].astype(str)

target = 'Churn'
df[target] = df[target].map({'Yes': 1, 'No': 0})

categorical_features = df.select_dtypes(include=['object']).columns.tolist()
numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_features.remove(target)

print(f"\nCategorical features: {categorical_features}")
print(f"Numerical features: {numerical_features}")

# --- 3. Prepare Data for Modeling ---
X = df.drop(target, axis=1)
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first')

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# --- 4. Model Training and Evaluation ---

def evaluate_model(name, model, X_test_processed, y_test):
    y_pred = model.predict(X_test_processed)
    y_proba = model.predict_proba(X_test_processed)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_proba)
    cm = confusion_matrix(y_test, y_pred)

    print(f"\n--- {name} Performance ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print("Confusion Matrix:")
    print(cm)

    # Store metrics for later comparison table
    metrics = {
        'Accuracy': accuracy, 'Precision': precision,
        'Recall': recall, 'F1-Score': f1, 'ROC AUC': roc_auc
    }
    return metrics, cm, y_proba
#class imbalance variable(mention in slides)
scale_pos_weight_value = (y_train == 0).sum() / (y_train == 1).sum()

# --- Model 1: Logistic Regression ---
lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('classifier', LogisticRegression(solver='liblinear',
                                                                class_weight='balanced',
                                                                random_state=42))])
lr_pipeline.fit(X_train, y_train)
lr_metrics, lr_cm, lr_proba = evaluate_model("Logistic Regression", lr_pipeline, X_test, y_test)

# --- Model 2: Gradient Boosting (XGBoost) ---
xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', XGBClassifier(random_state=42,
                                                            use_label_encoder=False,
                                                            eval_metric='logloss',
                                                            scale_pos_weight=scale_pos_weight_value))])
xgb_pipeline.fit(X_train, y_train)
xgb_metrics, xgb_cm, xgb_proba = evaluate_model("XGBoost", xgb_pipeline, X_test, y_test)

# --- 5. Results Visualization ---

# ROC Curves
plt.figure(figsize=(8, 6))
RocCurveDisplay.from_estimator(lr_pipeline, X_test, y_test, name='Logistic Regression', ax=plt.gca())
RocCurveDisplay.from_estimator(xgb_pipeline, X_test, y_test, name='XGBoost', ax=plt.gca())
plt.title('ROC Curves')
plt.savefig('roc_curves.png')
plt.show()

# Confusion Matrices Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
axes[0].set_title('Logistic Regression Confusion Matrix')
axes[0].set_xlabel('Predicted')
axes[0].set_ylabel('Actual')

sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1])
axes[1].set_title('XGBoost Confusion Matrix')
axes[1].set_xlabel('Predicted')
axes[1].set_ylabel('Actual')
plt.tight_layout()
plt.savefig('confusion_matrices.png')
plt.show()

# --- 6. Feature Importances ---

feature_names_out = lr_pipeline.named_steps['preprocessor'].get_feature_names_out()
lr_coefs = pd.DataFrame(
    lr_pipeline.named_steps['classifier'].coef_[0],
    index=feature_names_out,
    columns=['Coefficient']
).sort_values(by='Coefficient', key=abs, ascending=False)

print("\n--- Logistic Regression Coefficients ---")
print(lr_coefs.head(10))

plt.figure(figsize=(10, 8))
lr_coefs_top_10 = lr_coefs.head(10).sort_values(by='Coefficient', ascending=True)
lr_coefs_top_10['Coefficient'].plot(kind='barh')
plt.title('Top 10 Logistic Regression Coefficients (by absolute value)')
plt.xlabel('Coefficient Value (Log-Odds)')
plt.tight_layout()
plt.savefig('lr_coefficients.png')
plt.show()

xgb_importances = xgb_pipeline.named_steps['classifier'].feature_importances_
xgb_features_df = pd.DataFrame({
    'Feature': feature_names_out,
    'Importance': xgb_importances
}).sort_values(by='Importance', ascending=False)

print("\n--- XGBoost Feature Importances ---")
print(xgb_features_df.head(10))

plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=xgb_features_df.head(10), palette="viridis")
plt.title('Top 10 XGBoost Feature Importances')
plt.tight_layout()
plt.savefig('xgb_feature_importances.png')
plt.show()

# --- Summary Table for Presentation ---
summary_df = pd.DataFrame({
    'Logistic Regression': lr_metrics,
    'XGBoost': xgb_metrics
}).T
print("\n--- Model Performance Summary ---")
print(summary_df)
summary_df.to_csv('model_performance_summary.csv')

temp_churn_series = df['Churn']

churn_counts = temp_churn_series.value_counts()
churn_labels = ['No Churn', 'Churn']

plt.figure(figsize=(6, 6))
plt.pie(churn_counts, labels=churn_labels, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'salmon'])
plt.title('Customer Churn Distribution')
plt.savefig('churn_distribution_pie.png')
plt.show()

print(churn_counts)
print(f"Churn Percentage: {churn_counts[1] / len(df) * 100:.2f}%")